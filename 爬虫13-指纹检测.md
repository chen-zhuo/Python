# 指纹检测

指纹，这个很好理解，就是手指的纹路，每个人都有且独一无二，现实生活中很多案件都是靠嫌疑人的指纹破案的。**在写爬虫的时候，觉得只要自己每次请求都使用不同的代理 IP，每次请求的 Headers 都写得跟浏览器的一模一样，就不会被网站发现，但实际上还有一个东西，也叫做指纹。**

**这里的指纹，我们可以理解为一串带有特征字符的字符串。**ja3指纹查看网址 https://ja3er.com/json ，该网址会返回3个字段分别为：

```
"ja3_hash":ja3指纹哈希
"ja3":ja3指纹
"User-Agent":用户代理头
```

## 浏览器指纹

首先，**每种浏览器都有自己的指纹**，注意是**每种**，**也就意味着相同的浏览器的指纹是一样的**。

相同浏览器不同版本：**可以看到同样都是Chrome浏览器，一个92版本，一个96版本，但是他们访问，返回的指纹和哈希都是一样。**

![QQ截图20211210171139](image/QQ截图20211210171139.png)

不同浏览器：**可以看不同浏览器的指纹和哈希都是不一样的。**

![QQ截图20211210172014](image/QQ截图20211210172014.png)

## 爬虫指纹

**和浏览器类似，爬虫也有指纹，爬虫的指纹也不是唯一的。**

### 指纹测试

在不改变请求头的情况下访问该站点：**可以看到当只在一台机器上时，爬虫指纹是不会发生变化的；当换了一台机器后；是会发生变化的。**

```python
import requests

res = requests.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

res = requests.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

'''
第一台电脑输出：
3e185b41c1418c77e11390421fca3512
3e185b41c1418c77e11390421fca3512
第二胎电脑输出：
ed6dfd54b01ebe31b7a65b88abfa7297
ed6dfd54b01ebe31b7a65b88abfa7297
'''
```

在改变请求头的情况下访问该站点：**可以看到爬虫指纹没有发生变化。**

```python
import requests

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36"}
res = requests.get('https://ja3er.com/json', headers=headers)
print((res.json()).get("ja3_hash"))

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36 Edg/96.0.1054.43"}
res = requests.get('https://ja3er.com/json', headers=headers)
print((res.json()).get("ja3_hash"))

'''
输出：
3e185b41c1418c77e11390421fca3512
3e185b41c1418c77e11390421fca3512
'''
```

在使用代理的情况下访问该站点：**可以看到爬虫指纹没有发生变化。**

```python
import requests

proxies = {
    "http": "http://598020642:je3fy3ei@1.56.45.236:15977",
    "https": "https://598020642:je3fy3ei@1.56.45.236:15977"
}
res = requests.get('https://ja3er.com/json', proxies=proxies)
print((res.json()).get("ja3_hash"))

proxies = {
    "http": "http://598020642:je3fy3ei@106.116.65.132:20456",
    "https": "https://598020642:je3fy3ei@106.116.65.132:20456"
}
res = requests.get('https://ja3er.com/json', proxies=proxies)
print((res.json()).get("ja3_hash"))

'''
输出：
3e185b41c1418c77e11390421fca3512
3e185b41c1418c77e11390421fca3512
'''
```

在使用不同库的情况下访问该站点：**可以看到爬虫指纹发生变化。**

```python
import httpx
import requests

res = requests.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

res = httpx.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

'''
输出：
3e185b41c1418c77e11390421fca3512
f320b573a00b0a6ab6f3005486004da9
'''
```

这里可以得出一个结论就是：**爬虫的指纹是不会随着你访问次数、更换 IP 或者 User-Agent 而改变的，而是和使用的第三方库内部算法以及不同机器的客户端有关，但他们的指纹每次请求也是固定的。**

### JA3算法

上面的返回的JA3指纹是通过一种叫做**JA3算法**生成的，**该算法收集了 SSL 请求里面的信息，包括但不限于 SSL/TLS 版本，`Cipher Suites`数量，浏览器扩展列表，`elliptic curves`等等，通过这一系列参数综合起来生成一个指纹字符串。**也许这些参数里面，你跟一些人的 Cipher Suites 数量相同，你跟另外一些人的浏览器扩展数相同，你又跟另外一些人的 TLS 版本号相同……但是所有这些参数全部相同的人，就非常少了。而在这非常少的人里面，这些人还同时访问同一个网站的可能性就更小了。所以，网站用 JA3算法，可以近似认为，在一段时间内，指纹字符串相同的连续请求，有极大概率是来自同一个人。

使用Wireshark展示的Client Hello数据包示例：

![640](image/640.webp)

这些字段的顺序如下所示：

TLSVersion，Ciphers，Extensions，EllipticCurves，EllipticCurvePointFormats

例如：769,47–53–5–10–49161–49162–49171–49172–50–56–19–4,0–10–11,23–24–25,0

如果Client Hello数据包中没有TLS扩展（TLS Extensions），则这些字段的值为空。

例如：769,4–5–10–9–100–98–3–6–19–18–99,,,

然后，会计算这些字符串的MD5哈希值，以生成易于使用和共享的长度为32字符的指纹。它们就是JA3 TLS客户端的指纹。

例如：769,47–53–5–10–49161–49162–49171–49172–50–56–19–4,0–10–11,23–24–25,0 → ada70206e40642a3e4461f35503241d5
例如：769,4–5–10–9–100–98–3–6–19–18–99,,, → de350869b8c85de67a350c8d186f11e6

## 改变指纹

如果上面的地址不是一个测试网站，而是一个加了这个检测机制的网站，**只要网站发现某个拥有特定指纹的客户端持续高频率请求网站，那么它轻松就能把我给屏蔽了。为了让爬虫不被屏蔽，因此使用 requests 请求网站的时候，需要修改 JA3指纹。**

**通过上面的认识可知道 JA3 指纹里面，很大的一块就是 Cipher Suits，也就是加密算法。**而 requests 里面默认的加密算法如下：

![640 (18)](image/640 (18).webp)

```
ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:!eNULL:!MD5
```

**通过冒号分割了不同的加密算法，每一种顺序其实就对应了一个 JA3 指纹字符串，只要我们修改这个顺序，就能得到不同的JA3字符串。**

### DH改变指纹

回顾上一节《通信协议》中最后一个**反DH检测爬虫**就是修改了 requests 里面默认的加密算法，借用这个例子，我们来访问地址：**可以看到，爬虫指纹果然发生了变化，同时也说明了 加密算法和指纹的密切关系。**

```python
import requests

res = requests.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

# 反DH检测爬虫
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'
res = requests.get('https://ja3er.com/json')
print((res.json()).get("ja3_hash"))

'''
输出：
3e185b41c1418c77e11390421fca3512
a8fb57edced8c0a55d0268f819eed019
'''
```

### 生成指纹

前面讲到，指纹和加密算法息息相关，因此想要生成不同的指纹爬虫，就需要修改第三库的默认加密算法。**由于 requests 是基于 urllib3 实现的，要修改 Cipher Suits 中的加密算法，需要修改 urllib3 里面的 ssl 上下文，可以看看上面例子中调用的属性 `requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS` 。**

但在这里我们实现一个新的 HTTP 适配器 (HTTPAdapter)，在每次请求的时候，随机更换加密算法，**但需要注意的是`!aNULL:!eNULL:!MD5`就不用修改了，让他们保持在最后。**涉及到的代码如下：

```python
import random
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.ssl_ import create_urllib3_context

# requests默认的加密算法顺序
ORIGIN_CIPHERS = ('ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES')

class DESAdapter(HTTPAdapter):
    # 初始化，默认加密算法转为列表后打乱顺序重新组合
    def __init__(self, *args, **kwargs):
        """
        在一般情况下，当我们实现一个子类的时候，__init__的第一行应该是super().__init__(*args, **kwargs)，但是由于init_poolmanager和proxy_manager_for是复写了父类的两个方法，这两个方法是在执行super().__init__(*args, **kwargs)的时候就执行的。所以，我们随机设置 Cipher Suits 的时候，需要放在super().__init__(*args, **kwargs)的前面。
        """
        CIPHERS = ORIGIN_CIPHERS.split(':')
        random.shuffle(CIPHERS)
        CIPHERS = ':'.join(CIPHERS)
        self.CIPHERS = CIPHERS + ':!aNULL:!eNULL:!MD5'
        super().__init__(*args, **kwargs)
    
    def init_poolmanager(self, *args, **kwargs):
        context = create_urllib3_context(ciphers=self.CIPHERS)
        kwargs['ssl_context'] = context
        return super(DESAdapter, self).init_poolmanager(*args, **kwargs)

    def proxy_manager_for(self, *args, **kwargs):
        context = create_urllib3_context(ciphers=self.CIPHERS)
        kwargs['ssl_context'] = context
        return super(DESAdapter, self).proxy_manager_for(*args, **kwargs)
    
s = requests.Session()

for _ in range(5):
    # 适配器绑定到https://ja3er.com网址，表示适配器只在特定开头的网址中生效
    s.mount('https://ja3er.com', DESAdapter())
    res = s.get('https://ja3er.com/json')
    print((res.json()).get("ja3_hash"))

'''
输出：
30004cb53e14ceb7e9484fdabc63db86
f5b3199fb96caf32b1933874a3cd4c33
c3865e92e0e5c3b1c26aa0bb0da77493
ae56cffb397ef79fd9fb614c1be193e7
5ad9f93ea7cda1eb8f5a4429ac3f7277
'''
```

可以看到，`ja3_hash`已经改变了，说明我们请求时的 JA3 指纹已经发生了改变。

## 反指纹检测爬虫

下面的这道题名称为：指纹检测（提示本题会进行TLS检测，会屏蔽掉常用爬虫库且使用默认加密算法的爬虫），题目地址：https://match.yuanrenxue.com/match/，废话不多说，直接开干：

![640 (1)](image/6401.webp)

### 请求分析

看了下，没有加密参数：

![640 (2)](image/6402.webp)

然后拿着接口直接请求，没问题，直接返回结果：

![640 (3)](image/6403.webp)

用requests上，果然有猫腻：

![640 (4)](image/6404.webp)

那就只有抓包看看了，当我打开fiddler抓包时，再次运行程序，数据居然出来了：

![640 (5)](image/6405.webp)

但当我把fiddler一关，再次请求，又不行了：

![640 (6)](image/6406.webp)

那么思考下，开了fiddler跟没开fidder有什么区别？**最容易想到的代理证书的区别，fiddler伪造https证书，难道是ssl证书问题？我中途也试过请求时直接提交一个证书，也不行的。**还是先上个wireshark抓包吧：

?> 需要用最新版的wireshark才能看到ja3指纹，因为ja3指纹是基于tls1.3的，旧版的wireshark只能看到tls1.2及以下的。

这是浏览器请求一个接口的数据：

![640 (7)](image/6407.webp)

这是爬虫程序请求一次接口的数据：

![640 (8)](image/6408.webp)

一眼就能看出来区别，所以接下来就是分析区别了。先看浏览器的，选中那个【client Hello】包，展开最后发现了**ja3指纹算法**：

![640 (9)](image/6409.webp)

![640 (10)](image/64010.webp)

![640 (11)](image/64011.webp)

![640 (12)](image/64012.webp)

现在我们可以得出结论了：**也就是说，网站使用了ja3指纹检测，识别到了你用的python的请求库去请求，所以直接给你返回【page not found】。**

现在再看下python脚本的指纹，找到【client Hello】包，记录下指纹：

![640 (13)](image/64013.webp)

![640 (14)](image/64014.webp)

总结了下不同机制出现的ja3指纹：**从这个就可以很明显的发现不同了，凡是指纹很长的都是不通过的，为什么那么长，说明底层的加密算法用的不一样，这就是原因了。**

![640 (15)](image/64015.webp)

### 方法选择

规避检测大概有四种方法可以尝试：

1. **访问ip指定host绕过waf**：通过套了阿里云waf的服务器cname解析域名，这种情况可以直接ping域名获取真实ip，然后请求地址设置为真实ip在 HTTP Header的Host字段中指定域名即可绕过waf的防护，当然这种方式如果目标服务器开启了强制域名访问会失效。**经过测试发现不行，直接把域名替换成了host，仍然如此，说明不是cdn式的防护。**

2. **代理中转请求**：在本地启动代理服务器，如Burp Suite，发起http请求时指定代理服务器为burp的地址，让burp来进行TLS握手，算是一种曲线救国的方法。**这个在windows+fiddler可以，mac下不行，这里也就解释上面为啥开fiddler可行。**

3. **更换request工具库**：requests其实是对urllib3的一个封装，那python有没有不用urllib的http request库呢？**翻了翻aiohttp的源码发现貌似并没有用urllib3，抓包发现tls指纹和requests也有着明显的差异，但还是识别到并被拦截了下来。**

4. **魔改requests**：从根本上解决问题，debug跟踪到了几处可能可以修改TLS握手特征的代码：`/python安装路径/site-packages/urllib3/util/ssl_.py`。

那就只剩第四个方法了，我们**借用上面的生成指纹的方法**看看：

![640 (16)](image/64016.webp)

看ja3指纹的对比就知道，差太多了：

![640 (17)](image/64017.webp)

其原因就是：**用了random.shuffle来乱序算法，虽然每次出来都不是同一个ja3指纹，但指纹的长度并没有发生改变，而这里我们要欺骗服务器以为用的浏览器访问，而不是爬虫程序访问。**

### 指纹长度

既然长度不一样，那我删点默认的算法试试呢？以前搞过的那些js加密过后很长的加密字段，都是用了很多次加密算法得算法得出来的，所以我大胆猜测，还是加密算法用的太多了，我直接删好几个加密算法看看，我直接把下面选中都删了：

![640 (20)](image/64020.webp)

一执行，发现，可行了：

![640 (19)](image/64019.webp)

### 爬虫代码

爬虫代码如下：

```python
import re
import random
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.ssl_ import create_urllib3_context

headers = {
    'Host': 'match.yuanrenxue.com',
    'user-agent': 'yuanrenxue.project',
    'accept-encoding': 'gzip, deflate, br',
    'accept': 'application/json, text/javascript, */*; q=0.01',
    'Connection': 'keep-alive',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': '自己的cookie',
    'referer': 'https://match.yuanrenxue.com/match/19',
    'sec-ch-ua': '"Google Chrome";v="94", " Not;A Brand";v="99", "Chromium";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-User': '?1',
    'Sec-Fetch-Dest': 'document',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'x-requested-with': 'XMLHttpRequest'
}

# 魔改requests加密算法
ORIGIN_CIPHERS = ( 'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:RSA+3DES:!aNULL:!eNULL:!MD5'
)

class DESAdapter(HTTPAdapter):
    # 初始化，默认加密算法转为列表后打乱顺序重新组合
    def __init__(self, *args, **kwargs):
        """
        A TransportAdapter that re-enables 3DES support in Requests.
        """
        CIPHERS = ORIGIN_CIPHERS.split(':')
        random.shuffle(CIPHERS)
        CIPHERS = ':'.join(CIPHERS)
        self.CIPHERS = CIPHERS + ':!aNULL:!eNULL:!MD5'
        super().__init__(*args, **kwargs)

    def init_poolmanager(self, *args, **kwargs):
        context = create_urllib3_context(ciphers=self.CIPHERS)
        kwargs['ssl_context'] = context
        return super(DESAdapter, self).init_poolmanager(*args, **kwargs)

    def proxy_manager_for(self, *args, **kwargs):
        context = create_urllib3_context(ciphers=self.CIPHERS)
        kwargs['ssl_context'] = context
        return super(DESAdapter, self).proxy_manager_for(*args, **kwargs)

# 数值
values = 0
session = requests.Session()
# 共5页数据
for page in range(1, 6):
    # 接口地址
    url = f'https://match.yuanrenxue.com/api/match/19?page={page}'
    print(url)
    # 忽略掉警告
    requests.packages.urllib3.disable_warnings()
    session.mount(f'https://match.yuanrenxue.com/api/match/19?page={page}', DESAdapter())
    response = session.get(url, headers=headers, verify=False)
    print(f'第{page}页:{response.text}')
    for v in re.findall(r'{"value": (-?\d+)}', response.text):
        values += int(v)

# 总值
print(values)
```

这里多说一下，用**前面的DH改变指纹的方法**也是可以过该题目的，代码如下：

```python
import re
import requests
from requests.packages.urllib3.util.ssl_ import create_urllib3_context

headers = {
    'Host': 'match.yuanrenxue.com',
    'user-agent': 'yuanrenxue.project',
    'accept-encoding': 'gzip, deflate, br',
    'accept': 'application/json, text/javascript, */*; q=0.01',
    'Connection': 'keep-alive',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': '自己的cookie',
    'referer': 'https://match.yuanrenxue.com/match/19',
    'sec-ch-ua': '"Google Chrome";v="94", " Not;A Brand";v="99", "Chromium";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'Sec-Fetch-Site': 'none',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-User': '?1',
    'Sec-Fetch-Dest': 'document',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'x-requested-with': 'XMLHttpRequest'
}

# 数值
values = 0
session = requests.session()
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += 'HIGH:!DH:!aNULL'
# 共5页数据
for page in range(1, 6):
    # 接口地址
    url = f'https://match.yuanrenxue.com/api/match/19?page={page}'
    print(url)
    # 忽略掉警告
    requests.packages.urllib3.disable_warnings()
    response = session.get(url, headers=headers, verify=False)
    print(f'第{page}页:{response.text}')
    for v in re.findall(r'{"value": (-?\d+)}', response.text):
        values += int(v)

# 总值
print(values)
```



