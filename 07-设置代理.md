# 设置代理

### 什么是代理？

​	打个比方，有一活动，一张券换一桶油，但是一个人只能换一次。然而A有两张券，A去换了一桶油，假若A第二次去换油必然遭拒，这时A找到了B，把券给了B，让B去换油回来给A，这样A就换到了两桶油，B就可以说是A的代理。

​	**简单说：代理就是代替完成某项任务。**

### 为什么要使用代理？

​	当爬虫在正常的抓取数据时，会高频率的访问所爬取的网站。当网站服务器检测到这些访问来自同一IP时，若网站采取了反爬措施，服务器会直接拒绝服务（403 Forbidden），返回一些错误信息。这种情况就称为封IP，于是乎网站就成功把我们的爬虫禁掉了。

​	试想一下，我们借助某种方式来伪装 IP让服务器无法识别由我们本机发起的请求，这样不就可以成功防止封 IP 了吗？所以这时候代理就派上用场了。

### 设置代理

##### 获取代理

​	免费代理，比如西刺：http://www.xicidaili.com/ ，但是这些免费代理大多数情况下都是不好用的，**所以比较靠谱的方法是购买付费代理，付费代理在很多网站都有售卖，数量不用太多，稳定、可用即可。**

​	相关代理软件的话，一般会在本机创建 HTTP或SOCKS 代理服务。在这里，本机安装了一款代理软件，它会在本地 9743 端口上创建 HTTP 代理服务，即代理为127.0.0.1:9743 ，另外还会在 9742 端口创建 SOCKS 代理服务，即代理为 127.0.0.1:9742。只要设置了这个代理，就可以成功将本机IP切换到代理软件连接的服务器的IP了。

##### urllib设置代理

```
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

# 设置本机的代理端口9743
proxy = '127.0.0.1:9743'

# ProxyHandler设置代理，参数是字典类型，键名为协议类型，键值是代理。代理前面需要加上协议，即 http 或者 https。当请求的链接是http协议的时候，Proxy_Handler会调用http代理，当请求的链接是https协议的时候，会调用 https代理。
# 根据下面open中的url，此处生效的代理是http://127.0.0.1:9743
proxy_handler = ProxyHandler({
    'http ': 'http://' + proxy,
    'https': 'https://' + proxy
})
# 利用 build_opener（）方法传入该对象来创建 Opener,相当于Opener已经设置好代理，接下来直接调用 Opener对象的open（）方法， 即可访问想要的链接。
opener = build_opener(proxy_handler)

try:
    response = opener.open('http://httpbin.org/get')
    print(response.read().decode('utf 8'))
except URLError as e:
    print(e.reason)
    
    
结果：
{
  "args": {}, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Connection": "close", 
    "Host": "httpbin.org", 
    "User-Agent": "Python-urllib/3.6"
  }, 
  "origin": "117.139.208.10", 
  "url": "http://httpbin.org/get"
}
运行输出结果果是一个JSON，字段origin，标明了客户端的IP。验证一下，此处的IP确实为代理的IP，并不是真实的IP。这样我们就成功设置好代理，并可以隐藏真实。
```

##### requests设置代理

requests 的代理设置比 urllib 简单很多，它只需要构造代理字典，然后通过 proxies参数即可，而不需要重新构建 Opener。其运行结果的叫origin也是代理的 IP，这证明代理已经设置成功。

```
import requests

proxy = '127.0.0.1:9743'
proxies = {
    'http': 'http://' + proxy,
    'https': 'https://' + proxy,
}
try:
    response = requests.get('http://httpbin.org/get', proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error', e.args)

结果：
{
  "args": {}, 
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8", 
    "Accept-Encoding": "gzip, deflate", 
    "Accept-Language": "zh-CN,zh;q=0.9", 
    "Connection": "close", 
    "Host": "httpbin.org", 
    "Upgrade-Insecure-Requests": "1", 
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"
  }, 
  "origin": "117.139.208.10", 
  "url": "http://httpbin.org/get"
}
```

##### Selenium设置代理

```
from selenium import webdriver

proxy = '127.0.0.1:9743'
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--proxy-server=http://' + proxy)
browser = webdriver.Chrome(chrome_options=chrome_options)
browser.get('http://httpbin.org/get')

结果：
{
  "args": {}, 
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8", 
    "Accept-Encoding": "gzip, deflate", 
    "Accept-Language": "zh-CN,zh;q=0.9", 
    "Cache-Control": "max-age=0", 
    "Connection": "close", 
    "Host": "httpbin.org", 
    "Upgrade-Insecure-Requests": "1", 
    "User-Agent": "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"
  }, 
  "origin": "117.139.208.10", 
  "url": "http://httpbin.org/get"
}
```

### 代理池

​	利用代理可以解决目标网站封 IP 的问题。网上有大量公开的免费代理，或购买付费代理，但代理不论是免费的还是付费的，都不能保证都是可用的，因为可能 IP 被其他人使用来爬取同样的目标站点而被封禁，或者代理服务器突然发生故障或网络繁忙旦我们选用了一个不可用的代理，这势必会影响爬虫的工作效率。

​	所以，需要提前筛选，将不可用的代理剔除掉，保留可用代理，搭建一个高效易用的代理池。**简单说，代理池就是有效代理的集合。**

##### 四大模块

存储模块： 负责存储抓取下来的代理。

获取模块： 需要定时在各大代理网站抓取代理。

检测模块： 需要定时检测数据库中的代理。

接口模块： 需要用 API 来提供对外服务的接口。

Pychram运行ProxyPool-master文件夹中ProxyPool-master夹的run.py文件，将代理运行起来，代理池的控制台输出可以可以看到，可用代理设置为100，不可用代理分数减1。当前配置运行在5555端口，打开浏览器，打开http://127.0.0.1:5555 就可以看到首页‘Welcome to Proxy Pool System’，再访问http://127.0.0.1:5555/random ，即可获取随机一个可用代理。我们只需要访问此接口即可获取一个随机可用代理。

（注意：代理池检测的URL是www.baidu.com ，当爬取别的站点时，需要把ProxyPool-master文件夹中ProxyPool-master文件夹中proxypool文件夹中的setting.py文件里面的TEST_URL修改为目标站点的网址）

```
import requests

# 获取代理IP的接口
PROXY_POOL_URL = 'http://127.0.0.1:5555/random'
# 访问'http://127.0.0.1:5555/random'接口从代理池获取随机可用代理
def get_proxy():
    try:
        response = requests.get(PROXY_POOL_URL)
        if response.status_code == 200:
        	# 返回随机代理IP
            return response.text
    except ConnectionError:
        return None

# 使用get_proxy()方法获取一个随机代理
proxy = get_proxy()
proxies = {
    'http': 'http://' + proxy,
    'https': 'https://' + proxy,
}
# 使用代理IP打开http://httpbin.org/get测试连接，输出结果
try:
    response = requests.get('http://httpbin.org/get', proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error', e.args)

# 多次访问代理池接口
print(get_proxy())
print(get_proxy())
print(get_proxy())


结果：
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Connection": "close", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.19.1"
  }, 
  "origin": "46.173.191.51", 
  "url": "http://httpbin.org/get"
}
当中origin字段就是代理ip。

185.136.158.7:1080
189.195.162.222:53281
195.238.85.215:50948
每次访问代理池接口都会返回不同的代理IP
```

