# 异步并发

前面提到过启动多个爬虫不重复的爬取消息队列里面的链接，可以成倍的提高爬取效率，**其一是因为爬虫数量的增加提高了采集的效率；其二是因为减少了爬虫等待需要爬取链接的时间。**即便如此，爬虫效率的提升还有很大的空间，**因为现在我们写的爬虫是同步的，必须等待网页下载好才会执行下面的解析、入库操作**，如果在下载网页时间太长会导致阻塞。**假如把爬虫请求网页的部分剥离出来，只管去请求网页，下载、解析、入库等操作交给其他模块执行，这时爬虫就从同步变成了异步，效率更进一步提升。**

### 异步并发编程

##### 基础概念

**同步**：简单说，就是**对请求的结果进行等待**的执行方式。

**异步**：简单说，就是**不对请求的结果进行等待**的执行方式。

**并发**：简单说，就是**在一个时间段执行多个请求，任意时间点执行一个请求**的执行方式。

**并行**：简单说，就是**在一个时间段执行多个请求，任意时间点执行多个请求**的执行方式。

##### asyncio模块

**`asyncio`是Python 3.4版本引入的标准库，内置了对异步IO的支持，可以实现单线程并发IO操作。**

- `event_loop` 事件循环：程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数。
- `coroutine` 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。
- `task` 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含了任务的各种状态。
- `future`：代表将来执行或没有执行的任务的结果，本质上和task上没有的区别。
- `async/await` 关键字：python3.5用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。

`asyncio`编程模型：**从`asyncio`模块中直接获取一个`EventLoop`（事件循环）的引用，然后把需要执行的`coroutine`（协程）扔到`EventLoop`中执行，就实现了异步IO。**

```python
import asyncio

# async把hello()标记为coroutine（协程）类型
async def hello():
        print("Hello world!")
        # 线程挂起1秒
        await asyncio.sleep(1)
        print("Hello again!")

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# 把coroutine（协程）扔到EventLoop中执行
loop.run_until_complete(hello())
# 关闭事件循环
loop.close()

'''
输出：
Hello world!
（1秒后）
Hello again!
'''
```

##### 封装多协程

```python
import asyncio
import threading

async def hello():
        print('Hello world! (%s)' % threading.currentThread())
        await asyncio.sleep(1)
        print('Hello again! (%s)' % threading.currentThread())

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# tasks封装两个coroutine
tasks = [hello(), hello()]
loop.run_until_complete(asyncio.wait(tasks))
# 关闭事件循环
loop.close()

'''
输出：
Hello world! (<_MainThread(MainThread, started 140735195337472)>)
Hello world! (<_MainThread(MainThread, started 140735195337472)>)
(暂停约1秒)
Hello again! (<_MainThread(MainThread, started 140735195337472)>)
Hello again! (<_MainThread(MainThread, started 140735195337472)>)

解释1：把asyncio.sleep(1)看成是一个耗时1秒的IO操作，但主线程不会等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。

解释2：由打印的当前线程名称可以看出，两个coroutine是由同一个线程并发执行的。
'''
```

##### 对比多线程

将上面的程序，按照多线程的写法看看会有上面结果：

```python
import time
import threading

def run():
    print('Hello world! (%s)' % threading.currentThread())
    time.sleep(1)
    print('Hello again! (%s)' % threading.currentThread())

t1 = threading.Thread(target=run)
t2 = threading.Thread(target=run)
# 启动子线程t1、t2
t1.start()
t2.start()
# 阻塞主线程，等待子线程t1、t2执行结束
t1.join()
t2.join()
'''
输出：
Hello world! (<Thread(Thread-1, started 83108)>)
Hello world! (<Thread(Thread-2, started 86836)>)
(暂停约1秒)
Hello again! (<Thread(Thread-2, started 86836)>)
Hello again! (<Thread(Thread-1, started 83108)>)
'''
```

多线程程序中，根据线程ID可以看到该段程序运行过程中，生成了两个新的子线程，除此之外，运行的过程和效率都和多协程程序几乎一样，原因其实在前面早就给出答案：**因为GIL锁存在，两个线程看似同时在运行，实则是在交替运行，在任意一个时间点只有一个线程处于执行状态，也就是本该并行的多线程，被GIL强行转成并发的单线程。简单说，多线程可以看作是单线程并发，而多协程就是单线程并发。**

?> 因此许多人讲，Python的多线程实际上就是“伪多线程”。

### aiohttp框架

##### 简介

**aiohttp是一个基于asyncio实现的HTTP框架，它可以帮助我们异步地实现HTTP请求，从而使得我们的程序效率大大提高。** 

##### 优势

既然已经有requests了，那为什么还要说aiohttp了？

1. aiohttp是一个提供异步web服务的库，asyncio可以实现单线程并发IO操作。在python3.5中，加入了asyncio/await 关键字，使得回调的写法更加直观和人性化。

2. **requests，使用multiprocessing或者 threading加速爬虫也是一种方法。**

我们现在使用的aiohttp是异步的，简单来说，就是不需要等待，你尽管去下载网页就好了，我不用傻傻的等待你完成才进行下一步，我还有别的活要干。这样就极大的提高了下载网页的效率。

##### 信息爬取

```  
'''
异步方式爬取当当畅销书的图书信息
'''

import time
import aiohttp
import asyncio
import pandas as pd
from bs4 import BeautifulSoup

# table表格用于储存书本信息
table = []

# 获取网页（文本信息）
async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text(encoding='gb18030')

# 解析网页
async def parser(html):
    # 利用BeautifulSoup将获取到的文本解析成HTML
    soup = BeautifulSoup(html, "lxml")
    # 获取网页中的畅销书信息
    book_list = soup.find('ul', class_="bang_list clearfix bang_list_mode")('li')

    for book in book_list:
        info = book.find_all('div')
        # 获取每本畅销书的排名，名称，评论数，作者，出版社
        rank = info[0].text[0:-1]
        name = info[2].text
        comments = info[3].text.split('条')[0]
        author = info[4].text
        date_and_publisher = info[5].text.split()
        publisher = date_and_publisher[1] if len(date_and_publisher) >=2 else ''
        # 将每本畅销书的上述信息加入到table中
        table.append([rank,name,comments,author,publisher])

# 处理网页    
async def download(url):
    async with aiohttp.ClientSession() as session:
        # await的返回值就是协程运行的结果
        html = await fetch(session, url)
        await parser(html)

# 全部网页
urls = ['http://bang.dangdang.com/books/bestsellers/01.00.00.00.00.00-recent7-0-0-1-%d'%i for i in range(1,26)]

# 利用asyncio模块进行异步IO处理
# 创建事件循环
loop = asyncio.get_event_loop()
# asyncio.ensure_future创建任务
tasks = [asyncio.ensure_future(download(url)) for url in urls]
# asyncio.gather创建协程对象，列表task加*作用，将列表解开成每个独立的元素参数，传入函数
tasks = asyncio.gather(*tasks)
# 运行事件循环
loop.run_until_complete(tasks)

# 将table转化为pandas中的DataFrame并保存为CSV格式的文件
df = pd.DataFrame(table, columns=['rank','name','comments','author','publisher'])
df.to_csv('dangdang.csv',index=False)
```

##### 图片爬取

```
import requests
import os
import aiohttp
import asyncio
from fake_useragent import UserAgent

class Spider(object):
    def __init__(self):
        # 请求头
        self.headers = {'User-Agent':UserAgent().random}
        self.num = 1
        # 判断当前路径是否有名称为图片的文件夹
        if '图片' not in os.listdir('.'):
            os.mkdir('图片')
        # 确定绝对路径为图片文件夹下
        self.path = os.path.join(os.path.abspath('.'),'图片')
        os.chdir(self.path)

    # 获取图片url
    def get_img_links(self,page):
        url = 'https://unsplasj.com/napi/photos'
        data = {
            'page':page,
            'per_page':12,
            'order_by':'latest'
        }
        response = requests.get(url,params=data)
        # 构造后的url:https://https//unsplasj.com/napi/photos/?page=1&per_page=12&order_by=latest
        if response.status_code == 200:
            # 以json格式返回响应内容
            return response.json()
        else:
            print(f'请求失败，状态码为{response.status_code}')

    # 获取图片二进制内容
    async def get_content(self,link):
        async with aiohttp.ClientSession() as session:
            response = await session.get(link)
            # 获取图片内容
            content = await response.read()
            return content

    # 生成图片
    async def download_img(self,img):
        content = await self.get_content(img[1])
        with open(img[0]+'.jpg','wb') as f:
            f.write(content)
        print(f'下载第{self.num}张图片成功')

    def run(self):
        for x in range(1,101):
            loop = asyncio.get_event_loop()
            links = self.get_img_links(x)
            tasks = [asyncio.ensure_future(self.download_img((link['id'],link['links']['download']))) for link in links]
            loop.run_until_complete(asyncio.wait(tasks))


if __name__ == '__main__':
    spider = Spider()
    spider.run()
```

### 数据库异步操作

因为爬虫不仅仅只有下载这块，还会有操作数据库，这里提供两个异步库：`aioredis`、`motor`

##### aioredis

```
import asyncio
import aioredis

loop = asyncio.get_event_loop()

async def go():
   connect = await aioredis.create_connection('redis://localhost', loop=loop)
   await connect.execute('set', 'my-key', 'value')
   val = await connect.execute('get', 'my-key')
   print(val)
   conn.close()
   await conn.wait_closed()
loop.run_until_complete(go())
```

##### motor

```
import motor.motor_asyncio

client = motor.motor_asyncio.AsyncIOMotorClient('mongodb://localhost:27017')

db = client['test_database']
collection = db['test_collection']

async def do_insert():
   document = {'key': 'value'}
   result = await db.test_collection.insert_one(document)
   print('result %s' % repr(result.inserted_id))

async def do_find_one():
   document = await db.test_collection.find_one({'i': {'$lt': 1}})
   pprint.pprint(document)
```

