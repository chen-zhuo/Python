# 异步化

**在前面我们所写的爬虫程序同步的，这就导致爬取页面的机制是串行的，也就是说，必须等到第一个页面的访问、下载、采集、入库等一系列操作都完成以后，爬虫才会去爬取第二个页面，而在访问、下载过程中CPU资源是闲置的，而在采集、入库操作中网络资源是闲置的，假如某个步骤时间过长必然会延长采集时间，降低爬虫效率。**这个时候，使用异步的优势就显现了出来，**异步就是不对请求的结果进行等待的执行方式。**实现异步的方法有多种：**多进程、多线程、协程。**

这里提供一个测试网址：https://www.httpbin.org/delay/5，这个网站对每次响应增加了5秒延迟。为了更好的展示异步爬虫的优势，我们写不同的脚本分别爬取5次，比较其效率。

## 单线程爬虫

```python
import time
import requests

# 爬虫耗时装饰器
def timer(func):
    def deco():
        start = time.time()
        func()
        stop = time.time()
        print(f'爬虫耗时:{stop - start:.2f}秒!')
    return deco

def ssr4(times):
    response = requests.get('https://www.httpbin.org/delay/5')
    if response.status_code == 200:
        print(f'第{times}次爬取成功！')

@timer
def start():
    for times in range(1, 6):
        ssr4(times)

if __name__ == '__main__':
    start()

'''
输出：
第1次爬取成功！
第2次爬取成功！
第3次爬取成功！
第4次爬取成功！
第5次爬取成功！
爬虫耗时:30.78秒!
'''
```

## 多进程爬虫

```python
import time
import requests
from concurrent.futures import ProcessPoolExecutor

# 爬虫耗时装饰器
def timer(func):
    def deco():
        start = time.time()
        func()
        stop = time.time()
        print(f'爬虫耗时:{stop - start:.2f}秒!')
    return deco

def ssr4(times):
    response = requests.get('https://www.httpbin.org/delay/5')
    if response.status_code == 200:
        print(f'第{times}次爬取成功！')

@timer
def start():
    # 启用了进程池，最大工作单元为5个
    with ProcessPoolExecutor(max_workers=5) as pool:
        for times in range(1, 6):
            pool.submit(ssr4, times=times)

if __name__ == '__main__':
    start()

'''
输出：
第1次爬取成功！
第2次爬取成功！
第3次爬取成功！
第5次爬取成功！
第4次爬取成功！
爬虫耗时:20.56秒!
'''
```

## 多线程爬虫

```python
import time
import requests
from concurrent.futures import ThreadPoolExecutor

# 爬虫耗时装饰器
def timer(func):
    def deco():
        start = time.time()
        func()
        stop = time.time()
        print(f'爬虫耗时:{stop - start:.2f}秒!')
    return deco

def ssr4(times):
    response = requests.get('https://www.httpbin.org/delay/5')
    if response.status_code == 200:
        print(f'第{times}次爬取成功！')

@timer
def start():
    # 启用了线程池，最大工作单元为5个
    with ThreadPoolExecutor(max_workers=5) as pool:
        for times in range(1, 6):
            pool.submit(ssr4, times=times)

if __name__ == '__main__':
    start()

'''
输出：
第1次爬取成功！
第3次爬取成功！
第4次爬取成功！
第2次爬取成功！
第5次爬取成功！
爬虫耗时:7.01秒!
'''
```

!> 注意：单线程爬虫不用考虑资源竞争的问题，多线程爬虫需要考虑资源竞争、数据混乱和死锁的问题。

## aiohttp框架

前面的例子中，我们的请求都是由requests库发起的，**但由于requests库只能发送同步请求，并不支持异步 I/O，也就是说从向服务器请求网页到服务器响应并返回网页这一过程是同步的，如果在下载网页时间太长会导致阻塞，而且CPU性能和网络资源在这过程中也得不到充分的使用。**这时我们就需要考虑使用aiohttp框架了。

**aiohttp是一个基于asyncio和部分C语言实现的HTTP框架，它可以帮助我们以异步I/O的方式地实现HTTP请求，也就是说，我反正只管去发送网页的请求，不管服务器是否已经返回数据，这样使得我们的程序效率大大提高。** 安装 `aiohttp` 三方库，执行下面命令：

```bash
pip install aiohttp
```

?> 提示：aiohttp只支持3.5.3以后的Python版本。

### 基本使用

首先我们已经知道aiohttp是一个支持异步请求的库，那么它就必须配合支持异步I/O的asyncio模块使用，才能实现异步请求。

?> 提示：不清楚asyncio模块使用回看《程序运行概念、并发编程》中“协程”章节。

GET请求：

```python
import asyncio
import aiohttp

# 引入一个类aiohttp.ClientSession建立一个session对象，用session对象去打开网页
async with aiohttp.ClientSession() as session:
    # session.get即GET请求
    async with session.get(url) as response:
        # await的返回值就是协程运行的结果，执行response.text()才会真正发送请求。
        html = await response.text()
```

POST请求：

```python
import asyncio
import aiohttp

# 提交普通参数
async with aiohttp.ClientSession() as session:
    # session.post即POST请求，data即POST表单提交的参数
    async with session.post(url, data=data) as response:
        html = await response.text()

# 提交json参数
async with aiohttp.ClientSession() assession:
    # # session.post即POST请求，json即提交的json参数
    res = await session.post(url, json={'key': 'value'})
        result = await res.json()
```

设置代理：

```python
import asyncio
import aiohttp

async with aiohttp.ClientSession() as session:
    proxy = "http://124.113.251.151:8085"
    async with session.get(url, proxy=proxy) as response:
        html = await response.text()
```

!> 注意：aiohttp不支持https代理。

设置超时：

```python
import asyncio
import aiohttp

async def main():
    # 设置超时时间为10秒，如果超时，则抛出TimeoutError异常
    timeout = aiohttp.ClientTimeout(total=10)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get(url) as response:
            html = await response.text()
```

到这里能够发现，使用aiohttp和使用requests还是由明显区别的，主要包括如下几点：

- **首先除了导入aiohttp库外，还必须引入asyncio库。因为要实现异步爬取，需要启动协程，而协程则需要借助于asyncio里面的事件循环才能执行。**
- **每个异步方法的前面都要统一加 `async` 来修饰，包括 `with as` 语句，表明其是一个支持异步的上下文管理器。**
- **对于一些返回协程对象的操作，前面需要加 `await` 来修饰。通过查询API发现，response调用text方法其返回的是协程对象，那么前面就要加 `await` 来修饰，而对于状态码来说，其返回值就是一个数值，因此不需要加。**

### 多协程爬虫

这样我们可以将上面的爬虫例子用aiohttp来重写：

```python
import time
import asyncio
import aiohttp

async def ssr4(times):
    async with aiohttp.ClientSession() as session:
        url = 'https://www.httpbin.org/delay/5'
        async with session.get(url) as response:
            if response.status == 200:
                print(f'第{times}次爬取成功！')


if __name__ == '__main__':
    start = time.time()
    # 生成一个列表填充5个协程对象
    task = [ssr4(times) for times in range(1, 6)]
    # 生成事件循环对象loop
    loop = asyncio.get_event_loop()
    # asyncio.wait接受单个或者多个协程组成的列表，run_until_complete异步执行协程
    loop.run_until_complete(asyncio.wait(task))
    stop = time.time()
    print(f'爬虫耗时:{stop - start:.2f}秒!')

'''
第5次爬取成功！
第3次爬取成功！
第1次爬取成功！
第4次爬取成功！
第2次爬取成功！
爬虫耗时:6.79秒!
'''
```

这里再写一个更加复杂的爬虫：

```python
import time
import asyncio
import aiohttp

# 获取电影列表页
async def ssr4_page(page):
    async with ssr4_session.get(f'https://ssr1.scrape.center/page/{page}') as response:
        html = await response.text()
    return await ssr4_detail_page(html)

# 获取电影详情页
async def ssr4_detail_page(html):
    for url in re.findall(r'<a data-v-7f856186="" href="(.*?)"', html, re.S):
        while True:
            try:
                async with ssr4_session.get(f'https://ssr1.scrape.center{url}') as detail_response:
                    detail_html = await detail_response.text()
                movie_name = re.findall(r'<h2 data-v-63864230="" class="m-b-sm">(.*?)<', detail_html, re.S)[0].split(' - ')[0]
                print(movie_name, end='、')
                break
            except Exception as e:
                print(e)

def ssr4():
    # 首先定义一个全局ssr4_session，这样不需要再去各个方法里传递了
    global ssr4_session
    start = time.time()
    ssr4_session = aiohttp.ClientSession()
    task = [ssr4_page(page) for page in range(1, 11)]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(task))
    stop = time.time()
    print(f'\n爬虫耗时:{stop - start:.2f}秒!')

if __name__ == '__main__':
    ssr4()

'''
输出：上帝之城、当幸福来敲门、黄金三镖客、辩护人、哈利·波特与魔法石、海洋、活着、忠犬八公物语、幽灵公主、机器人总动员、V字仇杀队、我爱你、十二怒汉、指环王3：王者无敌、天堂电影院、阿飞正传、搏击俱乐部、指环王2：双塔奇兵、指环王1：护戒使者、射雕英雄传之东成西就、蝙蝠侠：黑暗骑士、海豚湾、飞屋环游记、7号房的礼物、疯狂原始人、英雄本色、哈利·波特与死亡圣器（下）、爱·回家、少年派的奇幻漂流、恐怖直播、甜蜜蜜、黑客帝国、无间道、黑客帝国3：矩阵革命、龙猫、窃听风暴、美丽心灵、教父2、剪刀手爱德华、七武士、初恋这件小事、速度与激情5、阿凡达、美国往事、加勒比海盗、时空恋旅人、借东西的小人阿莉埃蒂、春光乍泄、穿条纹睡衣的男孩、哈尔的移动城堡、完美的世界、一一、勇敢的心、大闹天宫、教父、蝙蝠侠：黑暗骑士崛起、盗梦空间、三傻大闹宝莱坞、美丽人生、鬼子来了、萤火之森、忠犬八公的故事、闻香识女人、天空之城、海上钢琴师、无敌破坏王、拯救大兵瑞恩、末代皇帝、素媛、霸王别姬、音乐之声、小鞋子、致命魔术、风之谷、熔炉、千与千寻、这个杀手不太冷、辛德勒的名单、神偷奶爸、迁徙的鸟、大话西游之大圣娶亲、魂断蓝桥、断背山、肖申克的救赎、怦然心动、新龙门客栈、大话西游之月光宝盒、驯龙高手、泰坦尼克号、放牛班的春天、触不可及、罗马假日、唐伯虎点秋香、钢琴家、乱世佳人、本杰明·巴顿奇事、喜剧之王、倩女幽魂、楚门的世界、狮子王、
爬虫耗时:11.99秒!
'''
```

?> 提示：详细测评参看：[浅度测评：requests、aiohttp、httpx 我应该用哪一个？](https://blog.csdn.net/u010467643/article/details/106132125)

### 高并发限制

由于aiohttp可以支持非常高的并发量，如几万、十万、百万都是能做到的，但面对如此高的并发量，目标网站可能无法在短时间内响应，而且由瞬间将目标网站爬挂掉的危险，这需要我们控制一下爬取的并发量。

一般情况下，可以借助asyncio的Semaphore来控制并发量，因此可以将上面代码修改为：

```python
import time
import asyncio
import aiohttp

# 借助Semaphore创建一个信号量对象，限制并发量为3个
semaphore = asyncio.Semaphore(3)

async def ssr4(times):
    # 有了信号量的控制，最大协程数量被限制在3个
    async with semaphore:
        async with aiohttp.ClientSession() as session:
            url = 'https://www.httpbin.org/delay/5'
            async with session.get(url) as response:
                await response.text()
                if response.status == 200:
                    print(f'第{times}次爬取成功！')


if __name__ == '__main__':
    start = time.time()
    task = [ssr4(times) for times in range(1, 6)]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(task))
    stop = time.time()
    print(f'爬虫耗时:{stop - start:.2f}秒!')

'''
输出：
第1次爬取成功！
第3次爬取成功！
第5次爬取成功！
第2次爬取成功！
第4次爬取成功！
爬虫耗时:12.62秒!

# 注释：可以看到这里的对并发进行了限制，采集时间就变成上面没有并发限制时间的2倍。
'''
```

## 总结

### 效率对比

经过上面的单线程、多进程、多线程、协程（不加并发限制）测试，时间对比如下：

```
单线程 - 爬虫耗时:30.78秒!
多进程 - 爬虫耗时:20.56秒!
多线程 - 爬虫耗时:7.01秒!
多协程 - 爬虫耗时:6.79秒!
```

根据爬虫的特点，按照任务类型分类，**爬虫属于"I/O密集型任务"**。针对“I/O密集型任务”解决方法就可以使用"多线程"或“多协程”，相较于其他方法的优势在于：

1. **单线程下有IO操作会进行IO等待，造成不必要的时间浪费。**
2. **多进程下会启动多个进程，时间花费较长，且资源开销大，但得益于并行执行，在效率方面高于单线程。**
2. **多线程使用的是一个进程，资源开销小，也因为多线程能解决很多IO阻塞问题，在效率方面比多进程高出一大截。**
2. **多协程使用的是一个线程，资源开销最小，且也省去了CPU在多个线程的来回调度和切换的时间，在效率方面高于多线程。**

### 归纳对比

从三方库使用来看：**requests库使用较为简单，但不支持异步请求；aiohttp库的使用稍复杂，且必须配合asyncio库来使用，但支持异步请求。**

从结果输出顺序看：**单线程爬虫的结果顺序任务顺序一致，多进程爬虫、多线程爬虫、多协程爬虫的结果顺序和任务顺序不一致。这是由于异步程序本身执行顺序不确定性造成的，其原理就是谁先处理完就输出谁。**简单说，异步意味着无序。
