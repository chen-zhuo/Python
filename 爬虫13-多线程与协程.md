# 多线程与协程

我们目前写的爬虫，看似爬取效率还可以，但实则效率并不高，这是因为我们所写的**爬虫本身是单线程的，这就导致爬取页面的机制是串行的，也就是说，必须等到第一个页面的访问、下载、采集、入库等一系列操作都完成以后，爬虫才会去爬取第二个页面，而在访问、下载过程中CPU资源是闲置的，而在采集、入库操作中网络资源是闲置的，假如某个操作时间过长必然会延长采集时间，降低爬虫效率。**这个时候，使用多线程和协程的优势就显现了出来。

## 基础回顾

为了更好的理解爬虫如何使用多线程和协程，需要回顾一下基础概念《[基础15-进程、线程、协程、效率](基础15-进程、线程、协程、效率.md)》

**同步**：简单说，就是**对请求的结果进行等待**的执行方式。

**异步**：简单说，就是**不对请求的结果进行等待**的执行方式。

**并发**：简单说，就是**在一个时间段执行多个请求，任意时间点执行一个请求**的执行方式。

**并行**：简单说，就是**在一个时间段执行多个请求，任意时间点执行多个请求**的执行方式。

**进程**：**计算机运行程序时产生的过程，同时也是CPU资源分配的最小单位。**

**线程**：**线程是对进程更小维度的划分，同时也是CPU调度的最小单位。**

**协程**：又称**微线程**，**是对线程更小维度的划分，可以简单的理解为多个相互协作的子程序。**

?> Python中可以使用多线程和多进程来实现并发，但还有一种实现并发的方式叫做异步编程，而协程就是实现异步编程的必要方式。

**I/O密集型任务**：**指磁盘I/O、网络I/O占主要的任务，输入输出量很大，计算量很小**（请求网页、 读写文件等）。

I/O密集型特点：**需要等待时间**，最适合方法就是，遇到延时操作时，就去执行其他的线程，即**多线程**。

**CPU密集型任务**：**计算密集型任务**，**指CPU计算占主要的任务，输入输出量很小，计算量很大**（复杂运算等）。

CPU密集型特点：**需要CPU的算力**，最适合方法就是，充分利用多核CPU，即**多进程**。

### 多线程优势

**根据爬虫的特点，按照任务类型分类，爬虫属于“I/O密集型任务”。**针对“I/O密集型任务”解决方法就是：多线程。比较相较于其他方法的优势在于：

1. **单线程下有IO操作会进行IO等待，造成不必要的时间浪费。**
2. **多线程能在线程A等待时，自动切换到线程B，减少浪费CPU的资源，从而能提升程序执行效率。**
3. **多进程利用 CPU 的多核优势，能同执行多个任务，但系统实现需要准备工作、耗费时间、资源开销大。**

### 单线程程序

单线程在程序运行上是**串行同步**的，**只有上一步程序运行完成后，才会运行下一步程序，否则一直等待**。

```python
import time

def hello():
    print('Hello world!')
    time.sleep(1)
    print('Hello again!')

for num in range(2):
    hello()

'''
输出：
Hello world!
（暂停1秒）
Hello again!
Hello world!
（暂停1秒）
Hello again!

解释：因为没有使用多线程或多进程，程序中只有一个执行单元，而time.sleep(1)的休眠操作会让整个线程停滞1秒钟，在这段时间里面CPU是完全闲置的没有做什么事情。
'''
```

### 多线程程序

将上面的程序，按照多线程的写法看看会有上面结果：

```python
import time
import threading

def run():
    print('Hello world! (%s)' % threading.currentThread())
    time.sleep(1)
    print('Hello again! (%s)' % threading.currentThread())

t1 = threading.Thread(target=run)
t2 = threading.Thread(target=run)
# 启动子线程t1、t2
t1.start()
t2.start()
# 阻塞主线程，等待子线程t1、t2执行结束
t1.join()
t2.join()
'''
输出：
Hello world! (<Thread(Thread-1, started 83108)>)
Hello world! (<Thread(Thread-2, started 86836)>)
(暂停约1秒)
Hello again! (<Thread(Thread-2, started 86836)>)
Hello again! (<Thread(Thread-1, started 83108)>)

解释1：多线程程序中，根据线程ID可以看到该段程序运行过程中，生成了两个新的子线程，两个子线程分别完整的运行了一次run函数。
解释2：两次运行run函数，共同等待了一秒，这说明CPU在一个线程执行遇到了延时操作time.sleep(1)，就切换到另一个线程上执行函数，减少了CPU闲置时间，提高了利用效率。
解释3：因为GIL锁存在，两个线程看似同时在运行，实则是在交替运行，在任意一个时间点只有一个线程处于执行状态，也就是本该并行的多线程，被GIL强行转成并发的单线程。因此许多人讲，Python的多线程实际上就是“伪多线程”。
'''
```

## 爬虫效率对比

为了更直观的看到多线程对爬虫效率的提升，下面比较单线程爬虫和多线程爬虫爬取相同内容的效率：

### 单线程爬虫

```python
'''
网址：爬取豆瓣豆瓣电影 Top 250
目的：测试单线程爬虫爬取250个URL效率
'''
import re
import time
import requests
from fake_useragent import UserAgent

# 电影详情网页
def parse(url):
    m_response = requests.get(url, headers=headers)
    m_webcode = re.sub(r'\s*', '', m_response.text)
    # 获取电影名字、评分、导演、语言
    try:
        m_name = re.search(r'name="title"value="(.*?)"', m_webcode).group(1)
        m_score = re.search(r'property="v:average">(.*?)<',m_webcode).group(1)
        m_director = re.search(r'rel="v:directedBy">(.*?)<', m_webcode).group(1)
        m_language = re.search(r'语言:</span>(.*?)<br/>', m_webcode).group(1)
        print(m_name, m_score, m_director, m_language)
    except:
        print('页面不存在')

if __name__ == '__main__':
    start_time = time.time()
    m_urls = []
    # 获取全部电影网页url
    urls = ['https://movie.douban.com/top250?start=%d&filter='%(i*25) for i in range(0,10)]
    for url in urls:
        headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
        response = requests.get(url, headers=headers)
        webcode = re.sub(r'\s*', '', response.text)
        for item in re.findall(r'<divclass="item">.*?</div>', webcode):
            m_href = re.search(r'href="(.*?)"', item).group(1)
            m_urls.append(m_href)
    # 爬取250部电影
    for m_url in m_urls:
        parse(m_url)
    end_time = time.time()
    print(f'花费时间：{end_time-start_time}') 
'''
输出：
肖申克的救赎TheShawshankRedemption‎(1994) 9.7 弗兰克·德拉邦特 英语
...
黑客帝国2：重装上阵TheMatrixReloaded‎(2003) 8.6 莉莉·沃卓斯基 英语/法语
花费时间：224.8161380290985（爬取三次取平均值）
'''
```

### 多线程爬虫

```python
'''
网址：爬取豆瓣豆瓣电影 Top 250
目的：测试多线程爬取250个URL的效率
'''
import re
import time
import threading
import requests
from fake_useragent import UserAgent

# 创建一个线程锁对象lock
lock = threading.Lock()

# 电影详情网页
def parse(url):
    headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
    response = requests.get(url, headers=headers)
    m_webcode = re.sub(r'\s*', '', response.text)
    # 加锁是为了防止多线程输出混乱
    with lock:
        # 获取电影名字、评分、导演、语言
        try:
            m_name = re.search(r'name="title"value="(.*?)"', m_webcode).group(1)
            m_score = re.search(r'property="v:average">(.*?)<',m_webcode).group(1)
            m_director = re.search(r'rel="v:directedBy">(.*?)<', m_webcode).group(1)
            m_language = re.search(r'语言:</span>(.*?)<br/>', m_webcode).group(1)
            print(m_name, m_score, m_director, m_language)
        except:
            print('页面不存在')

if __name__ == '__main__':
    start_time = time.time()
    m_urls = []
    # 获取全部电影网页url
    urls = ['https://movie.douban.com/top250?start=%d&filter='%(i*25) for i in range(0,10)]
    for url in urls:
        headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
        response = requests.get(url, headers=headers)
        webcode = re.sub(r'\s*', '', response.text)
        for item in re.findall(r'<divclass="item">.*?</div>', webcode):
            m_href = re.search(r'href="(.*?)"', item).group(1)
            m_urls.append(m_href)
    # 定义一个列表，向里面追加线程
    threads = []
    for m_url in m_urls:
        # 注意args()里面的内容必须是元组，只有一个元素时，必须要加上逗号
        t = threading.Thread(target=parse, args=(m_url,))
        threads.append(t)
    for t in threads:
        t.start()  # 调用start()方法，开始执行
    for t in threads:
        t.join()  # 子线程调用join()方法，使主线程等待子线程运行完毕之后才退出
    end_time = time.time()
    print(f'花费时间：{end_time-start_time}')
'''
输出：
页面不存在（'电影"V字仇杀队"网页失效'）
...
聚焦Spotlight‎(2015) 8.8 汤姆·麦卡锡 英语
花费时间：11.488420009613037（爬取三次取平均值）
'''
```

### 效率对比总结

通过上面单线程爬虫和多线程爬虫的爬取用时和输出结果，可得出如下结论：

1. **多线程爬虫效率更高，因为多线程能解决很多IO阻塞问题，而且爬取的数量越大，效率提升的就越明显。**
2. **单线程爬虫的结果输出顺序和网页的排版顺序一致，多线程爬虫的结果输出顺序则是谁先处理完就输出谁。**
3. **单线程爬虫不用考虑资源竞争的问题，多线程爬虫需要考虑资源竞争、数据混乱和死锁的问题。**

## 协程优势

前面讲过得益于多线程能解决很多IO阻塞问题这一特性，多线程爬虫比单线程爬虫，在效率上有了很大的提高，但即便如此，爬虫效率还有提升的空间，这是因为**我们现在用的requests库是一个同步库，也就是说从向服务器请求网页到服务器响应并返回网页这一过程是同步的，如果在下载网页时间太长会导致阻塞，而且CPU性能和网络资源在这过程中也得不到充分的使用。**

对比前面例子中的“单线程”，“多线程”，协程的优势在于：

1. **在同一个线程中，一个协程阻塞时，马上切换到另一个协程，提升CPU利用率，用协作的方式加速程序执行。**
2. **相比于多线程，内存开销更小，自由切换，避免了无意义的调度。**
3. **因为只有一个线程，不需要锁机制。**

## asyncio模块

**`asyncio`是Python 3.4版本引入的标准库，内置了对异步IO的支持，可以实现单线程并发IO操作。**

- `event_loop` 事件循环：程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数。
- `coroutine` 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。
- `task` 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含了任务的各种状态。
- `future`：代表将来执行或没有执行的任务的结果，本质上和task上没有的区别。
- `async/await` 关键字：python3.5用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。

`asyncio`编程模型：**从`asyncio`模块中直接获取一个`EventLoop`（事件循环）的引用，然后把需要执行的`coroutine`（协程）扔到`EventLoop`中执行，就实现了异步IO。**

### 单协程程序

```python
import asyncio

# async把hello()标记为coroutine（协程）类型，这种函数最大特点是执行可以暂停，交出执行权
async def hello():
        print("Hello world!")
        # 在async函数内部加上await命令生成异步任务，线程挂起1秒
        await asyncio.sleep(1)
        print("Hello again!")

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# 把coroutine（协程）扔到EventLoop中执行
loop.run_until_complete(hello())
# 关闭事件循环
loop.close()

'''
输出：
Hello world!
（1秒后）
Hello again!

解释1：执行引擎遇到await命令，暂停当前async函数的执行，把执行权交给其他任务。等到异步任务结束，再把执行权交回async函数，继续往下执行。
解释2：coroutine（协程）不能直接被运行，必须要封装为task或者future才能被运行，而run_until_complete方法可以自动将协程封装为task任务。
'''
```

### 多协程程序

```python
import asyncio
import threading

# async把hello()标记为coroutine（协程）类型
async def hello():
        print('Hello world! (%s)' % threading.currentThread())
        await asyncio.sleep(1)
        print('Hello again! (%s)' % threading.currentThread())

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# tasks封装三个coroutine
tasks = [hello(), hello(), hello()]
# wait()方法作用是接受多个协程组成列表
loop.run_until_complete(asyncio.wait(tasks))
# 关闭事件循环
loop.close()

'''
输出：
Hello world! (<_MainThread(MainThread, started 140735195337472)>)
Hello world! (<_MainThread(MainThread, started 140735195337472)>)
Hello world! (<_MainThread(MainThread, started 140735195337472)>)
(暂停约1秒)
Hello again! (<_MainThread(MainThread, started 140735195337472)>)
Hello again! (<_MainThread(MainThread, started 140735195337472)>)
Hello again! (<_MainThread(MainThread, started 140735195337472)>)

解释1：把asyncio.sleep(1)看成是一个耗时1秒的IO操作，但协程对象不会等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发执行。这也就说明协程对象一旦阻塞会将CPU让出而不是让CPU处于闲置状态，这样就大大的提升了CPU的利用率。
解释2：由打印的当前线程名称可以看出，三个coroutine协程是由同一个线程并发执行的。
'''
```

?> 因为GIL锁的存在，简单说，多线程在I/O阻塞时通过切换线程来达到并发的效果，而多协程在I/O阻塞时通过在线程内切换任务来达到并发的效果。

### `run` 方法

在Python3.7以前的版本，调用异步函数前要先调用 `asyncio.get_event_loop()` 函数获取事件循环loop对象，然后通过 `loop.run_until_complete()` 方法执行异步函数，也就是上面的两个程序例子。但在**Python3.7的正式版本**添加了一些新API，**例如 `asyncio.run()` 方法，可以省去显式的定义事件循环的步骤。**

将多协程例子重写为：

```python
import asyncio
import threading

async def hello():
    print('Hello world! (%s)' % threading.currentThread())
    await asyncio.sleep(1)
    print('Hello again! (%s)' % threading.currentThread())

async def main():
    # gather()方法将三个hello()异步任务包装成一个异步任务，必须等到内部多个异步任务都执行结束，这个新异步任务才会结束。
    await asyncio.gather(hello(), hello(), hello())

asyncio.run(main())

'''
输出：
Hello world! (<_MainThread(MainThread, started 7544)>)
Hello world! (<_MainThread(MainThread, started 7544)>)
Hello world! (<_MainThread(MainThread, started 7544)>)
（1秒后）
Hello again! (<_MainThread(MainThread, started 7544)>)
Hello again! (<_MainThread(MainThread, started 7544)>)
Hello again! (<_MainThread(MainThread, started 7544)>)
'''
```

### 方法详解

上面不同的例子中用到了：`asyncio.wait` 方法和 `asyncio.gather` 方法。重点解释一下：

**coroutine（协程）不能直接被运行，必须要封装为 `task任务` 或者 `future` 才能被被运行。**

**`asyncio.wait` 方法作用是接受单个或者多个协程组成的列表`[coroutine, coroutine...]`，但是包含协程的列表也不能直接被运行，而 `run_until_complete` 方法会首先将tasks列表里的协程先转换为future，所以需要配合使用。**

**`asyncio.gather` 方法作用是接受单个或者多个协程组成的列表`[coroutine, coroutine...]`，但是会将列表中不是task的协程预先封装为future。**

事件循环的 `run_until_complete` 方法对协程对象注册时就自动把它封装成了一个 `task`，我们也可以**显式的将协程对象包装成一个 `task` 再将其注册**。对协程对象包装成task有两种方法：

1. 用事件循环的 `create_task` 方法

```python
import asyncio

async def hello():
        print("Hello world!")
        await asyncio.sleep(1)
        print("Hello again!")

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# create_task方法对协程对象包装成task
task = loop.create_task(hello())
# 把coroutine（协程）扔到EventLoop中执行
loop.run_until_complete(task)
# 关闭事件循环
loop.close()
```

2. 用 asyncio 的 `ensure_future` 方法

```python
import asyncio

# async把hello()标记为coroutine（协程）类型，这种函数最大特点是执行可以暂停，交出执行权
async def hello():
        print("Hello world!")
        # 在async函数内部加上await命令生成异步任务，线程挂起1秒
        await asyncio.sleep(1)
        print("Hello again!")

# 通过get_event_loop()建立事件循环
loop = asyncio.get_event_loop()
# ensure_future方法对协程对象包装成task
task = asyncio.ensure_future(hello())
# 把coroutine（协程）扔到EventLoop中执行
loop.run_until_complete(task)
# 关闭事件循环
loop.close()
```

## aiohttp框架

关于aiohttp框架的文档参看：[aiohttp官方文档](https://docs.aiohttp.org/en/stable/)

### 框架简介

前面提到requests是同步的，也就是说，从向服务器请求网页开始一会直等到服务器响应并返回网页。

**aiohttp是一个基于asyncio实现的HTTP框架，它可以帮助我们异步地实现HTTP请求，也就是说，我反正只管去发送网页的请求，不管服务器是否已经返回数据，这样使得我们的程序效率大大提高。** 

### aiohttp请求方式

GET请求：

```python
# 引入一个类aiohttp.ClientSession建立一个session对象，用session对象去打开网页
async with aiohttp.ClientSession() as session:
    # session.get(url)即GET请求
    async with session.get(url) as response:
        # await的返回值就是协程运行的结果
        html = await response.text()
```

POST请求：

```python
# 提交普通参数
async with aiohttp.ClientSession() as session:
    # session.post(url,data)即POST请求，data即POST表单提交的参数
    async with session.post(url, data=data) as response:
        # await的返回值就是协程运行的结果
        html = await response.text()

# 提交json参数
async with aiohttp.ClientSession() assession:
    res = await session.post('url', json={'key': 'value'})
        # 当使用aiohttp时，只有在awiat resp.json() 时才会真正发送请求。
        result = await res.json()
```

### aiohttp设置代理

requests设置代理的方式：`proxies设置代理的参数`**复数形式**

```python
proxies = {
            'http': 'http://124.113.251.151:8085',
            'https': 'https://124.113.251.151:8085',
        }
response = requests.get('URL', proxies=proxies)
```

aiohttp设置代理的方式：`proxy设置代理的参数`**单数形式**

```python
async with aiohttp.ClientSession() as session:    async with session.get(url, proxy=f"http://124.113.251.151:8085") as response:        # await的返回值就是协程运行的结果        html = await response.text()
```

### requests同步爬虫

```python
'''
网址：爬取豆瓣豆瓣电影 Top 250
目的：测试requests爬取250个URL
'''
import re
import time
import requests
from fake_useragent import UserAgent

# 电影详情网页
def parse(url):
    m_response = requests.get(url, headers=headers)
    m_webcode = re.sub(r'\s*', '', m_response.text)
    # 获取电影名字、评分、导演、语言
    try:
        m_name = re.search(r'name="title"value="(.*?)"', m_webcode).group(1)
        m_score = re.search(r'property="v:average">(.*?)<',m_webcode).group(1)
        m_director = re.search(r'rel="v:directedBy">(.*?)<', m_webcode).group(1)
        m_language = re.search(r'语言:</span>(.*?)<br/>', m_webcode).group(1)
        print(m_name, m_score, m_director, m_language)
    except:
        print('页面不存在')

if __name__ == '__main__':
    start_time = time.time()
    m_urls = []
    # 获取全部电影网页url
    urls = ['https://movie.douban.com/top250?start=%d&filter='%(i*25) for i in range(0,10)]
    for url in urls:
        headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
        response = requests.get(url, headers=headers)
        webcode = re.sub(r'\s*', '', response.text)
        for item in re.findall(r'<divclass="item">.*?</div>', webcode):
            m_href = re.search(r'href="(.*?)"', item).group(1)
            m_urls.append(m_href)
    # 爬取250部电影
    for m_url in m_urls:
        parse(m_url)
    end_time = time.time()
    print(f'花费时间：{end_time-start_time}') 
'''
输出：
肖申克的救赎TheShawshankRedemption(1994) 9.7 弗兰克·德拉邦特 英语
...
黑客帝国2：重装上阵TheMatrixReloaded(2003) 8.6 莉莉·沃卓斯基 英语/法语
花费时间：224.8161380290985（爬取三次取平均值）
'''
```

### aiohttp异步爬虫

```python
'''
网址：爬取豆瓣豆瓣电影 Top 250
目的：测试aiohttp爬取250个URL
'''
import re
import time
import asyncio
import aiohttp
import requests
from fake_useragent import UserAgent

# 电影详情网页
# async标记为协程对象
async def parse(url):
    # 引入一个类aiohttp.ClientSession建立一个session对象，用session对象去打开网页
    async with aiohttp.ClientSession() as session:
        # session.get(url)即GET请求，session.post(url,data)即POST请求，
        async with session.get(url) as response:
            # await的返回值就是协程运行的结果
            m_webcode = re.sub(r'\s*', '', await response.text())
            # 获取电影名字、评分、导演、语言
            try:
                m_name = re.search(r'name="title"value="(.*?)"', m_webcode).group(1)
                m_score = re.search(r'property="v:average">(.*?)<',m_webcode).group(1)
                m_director = re.search(r'rel="v:directedBy">(.*?)<', m_webcode).group(1)
                m_language = re.search(r'语言:</span>(.*?)<br/>', m_webcode).group(1)
                print(m_name, m_score, m_director, m_language)
            except:
                print('页面不存在')

if __name__ == '__main__':
    start_time = time.time()
    m_urls = []
    # 获取全部电影网页url
    urls = ['https://movie.douban.com/top250?start=%d&filter='%(i*25) for i in range(0,10)]
    for url in urls:
        headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
        response = requests.get(url, headers=headers)
        webcode = re.sub(r'\s*', '', response.text)
        for item in re.findall(r'<divclass="item">.*?</div>', webcode):
            m_href = re.search(r'href="(.*?)"', item).group(1)
            m_urls.append(m_href)
    # 创建事件循环
    loop = asyncio.get_event_loop()
    # asyncio.ensure_future创建任务
    tasks = [asyncio.ensure_future(parse(m_url)) for m_url in m_urls]
    # asyncio.gather创建协程对象，列表task加*作用，将列表解开成每个独立的元素参数，传入函数
    tasks = asyncio.gather(*tasks)
    # 运行事件循环
    loop.run_until_complete(tasks)
    end_time = time.time()
    print(f'花费时间：{end_time-start_time}') 
'''
输出：
页面不存在（'电影"V字仇杀队"网页失效'）
...
消失的爱人GoneGirl(2014) 8.7 大卫·芬奇 英语
花费时间：10.334077835083008（爬取三次取平均值）

解释1：从结果输出顺序可以看出，协程对象创建顺序不同于输出顺序，这是由于并发程序本身执行顺序不确定性造成的。
解释2：相比于多线程爬虫，异步爬虫性能略优，因为没有了线程间的来回切换再加上aiohttp异步请求优势。
'''
```

?> 详细测评参看：[浅度测评：requests、aiohttp、httpx 我应该用哪一个？](https://blog.csdn.net/u010467643/article/details/106132125)

### 高并发异常解决

上面的例子中测试aiohttp爬取250个URL的效率，但如果URL的超过2000个，程序会报错：`ValueError: too many file descriptors in select()`。报错的原因字面上看是 Python 调取的 select 对打开的文件有最大数量的限制，这个其实是操作系统的限制，**linux打开文件的最大数默认是1024，windows默认是509，超过了这个值，程序就开始报错。**这里我们有**三种方法解决**这个问题：

**1.限制并发数量**。（一次不要塞那么多任务，或者限制最大并发数量）

**2.使用回调的方式**。

**3.修改操作系统打开文件数的最大限制，在系统里有个配置文件可以修改默认值，具体步骤不作说明了。**

个人推荐限制并发数的方法，设置并发数为500，处理速度更快。可以将上面代码修改为：

```python
import re
import time
import asyncio
import aiohttp
import requests
from fake_useragent import UserAgent

# 电影详情网页
# async标记为协程对象
async def parse(url, semaphore):
    async with semaphore:
        # 引入一个类aiohttp.ClientSession建立一个session对象，用session对象去打开网页
        async with aiohttp.ClientSession() as session:
            # session.get(url)即GET请求，session.post(url,data)即POST请求，
            async with session.get(url) as response:
                # await的返回值就是协程运行的结果
                m_webcode = re.sub(r'\s*', '', await response.text())
                # 获取电影名字、评分、导演、语言
                try:
                    m_name = re.search(r'name="title"value="(.*?)"', m_webcode).group(1)
                    m_score = re.search(r'property="v:average">(.*?)<',m_webcode).group(1)
                    m_director = re.search(r'rel="v:directedBy">(.*?)<', m_webcode).group(1)
                    m_language = re.search(r'语言:</span>(.*?)<br/>', m_webcode).group(1)
                    print(m_name, m_score, m_director, m_language)
                except:
                    print('页面不存在')

if __name__ == '__main__':
    start_time = time.time()
    m_urls = []
    # 获取全部电影网页url
    urls = ['https://movie.douban.com/top250?start=%d&filter='%(i*25) for i in range(0,10)]
    for url in urls:
        headers = {'User-Agent': UserAgent(use_cache_server=False).chrome}
        response = requests.get(url, headers=headers)
        webcode = re.sub(r'\s*', '', response.text)
        for item in re.findall(r'<divclass="item">.*?</div>', webcode):
            m_href = re.search(r'href="(.*?)"', item).group(1)
            m_urls.append(m_href)
    # 创建事件循环
    loop = asyncio.get_event_loop()
    # 限制并发量为500
    semaphore = asyncio.Semaphore(500)
    # 封装多个协程
    tasks = [parse(m_url, semaphore) for m_url in m_urls]
    # 运行事件循环
    loop.run_until_complete(asyncio.gather(*tasks))
    end_time = time.time()
    print(f'花费时间：{end_time-start_time}')
```

## 数据库异步

爬虫不仅仅只有访问下载这块，还有异步存储，这里提供两个异步库：`aioredis`、`motor`

### aioredis

```python
import asyncio
import aioredis

loop = asyncio.get_event_loop()

async def go():
    # 连Redis接数据库
   connect = await aioredis.create_connection('redis://localhost', loop=loop)
   await connect.execute('set', 'my-key', 'value')
   val = await connect.execute('get', 'my-key')
   print(val)
   conn.close()
   await conn.wait_closed()
loop.run_until_complete(go())
```

### motor

```python
import motor.motor_asyncio

# 连接MongoDB数据库
client = motor.motor_asyncio.AsyncIOMotorClient('mongodb://localhost:27017')

db = client['test_database']
collection = db['test_collection']

async def do_insert():
   document = {'key': 'value'}
   result = await db.test_collection.insert_one(document)
   print('result %s' % repr(result.inserted_id))

async def do_find_one():
   document = await db.test_collection.find_one({'i': {'$lt': 1}})
   pprint.pprint(document)
```

